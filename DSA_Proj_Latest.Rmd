---
title: "DSA301 Project"
author: "Everyone"
output: html_document
---
Questions:
1. Decompose first or transform first (benchmark models)
3. How to read the PACF & ACF graphs? Currently just using guess and check from changing models provided by auto.arima

##Loading Packages
```{r cars, include=FALSE}
rm(list = ls())
library(jsonlite)
library(lubridate)
library(forecast)
library(urca)
library(TSstudio)
library(prophet)
library(randomForest)
library(keras) # for deep learning
library(tidyverse) # general utility functions
library(caret) # machine learning utility functions
library(randomForest) # for RF modelling 
source("dataprocessing.R")
```

### Reading CSV, Processing into TS Object, Visualisation of Data
```{r}
daily_data = read_csv('trainvaldf.csv', show_col_types = FALSE)
# train = get_train_df(daily_data)
# val = get_val_df(daily_data)

# Putting data into TS Object
msts_wiki_daily = daily_data$views %>% msts(seasonal.periods = c(7, 365), start =c(2015, as.numeric(format(daily_data$date[1], "%j"))))

# Creating Train and Test Set
train_size = length(daily_data$date[year(daily_data$date) < 2021])
val_size = dim(daily_data)[1] - train_size

msts_split <- ts_split(msts_wiki_daily, sample.out = val_size)
msts_train = msts_split$train
msts_test = msts_split$test
```

###Visualising STL decomposition
```{r}
daily_train_decomp = mstl(msts_train)
seasadj_wiki <- seasadj(daily_train_decomp)
autoplot(daily_train_decomp)
# Both the yearly seasonal pattern and the weekly seasonal pattern seem significant (in terms of impact on overall data), and therefore we do not rule any of them out at the moment.
```
## Differencing Seasonal Adjusted Component
```{r}
#No differencing: Test stat = 5
# new = 6.483
  seasadj_wiki %>% ur.kpss() %>% summary()

#Test stat = 0.011
# new = 0.0102
seasadj_wiki %>% 
  diff(lag =1)%>%
  ur.kpss()%>%
  summary()

#Test stat = 0.0142 
# new = 0.0137
seasadj_wiki %>% 
  diff(lag =7)%>%
  ur.kpss()%>%
  summary()

#Test stat = 0.003
# new = 0.0026
seasadj_wiki %>% 
  diff(lag =7)%>% 
  diff(lag = 1)%>%
  ur.kpss()%>%
  summary()

#Test stat = 2.2636
# new = 2.2177
seasadj_wiki %>% 
  diff(lag = 365)%>%
  ur.kpss()%>%
  summary()

#Test stat = 0.0143
# new = 0.0137
seasadj_wiki %>% 
  diff(lag =365)%>% 
  diff(lag = 1)%>%
  ur.kpss()%>%
  summary()
```


##Benchmark Models
###Benchmark Model Creation
```{r}
b1 <- stlf(msts_train, method = "naive", robust = T, h = val_size)
b2 <- stlf(msts_train, method = "rwdrift", robust = T, h = val_size)
checkresiduals(b1)
checkresiduals(b2)
accuracy(b1, msts_test)
accuracy(b2, msts_test)
# b1 RMSE - 17.005684
# b2 RMSE - 17.835712

# Ljung Box Test - We reject the null hypothesis that residuals are independently distributed, and we conclude that there is serial correlation. 
```
### Plotting Results
```{r}
autoplot(b1)
autoplot(b2)
# TRY TO PLOT THIS OUT
```

##ARIMA Models
###ACF & PACF Analysis for Differenced & Undifferenced Seasonal Adjusted Component
```{r}
# These two approaches to differencing yielded the lowest p value for the KPSS test.
trans_seasadj_wiki_1 <- seasadj_wiki %>%diff(lag = 1)
trans_seasadj_wiki_2 <- seasadj_wiki %>%diff(lag = 7)%>%diff(lag = 1)
autoplot(trans_seasadj_wiki_1)
autoplot(trans_seasadj_wiki_2)

ggtsdisplay(seasadj_wiki)
pacf(seasadj_wiki)
acf(seasadj_wiki)
# ACF is slowly decreasing. This means that there is high autocorrelation with each lagged variable, suggesting that there is a unit room in the data and it is not stationary. This confirms what we had found earlier. We will look at the ACF and PACF of the variable based on the taking the difference, suggested by the KPSS test.

ggtsdisplay(trans_seasadj_wiki_1)
pacf(trans_seasadj_wiki_1)
acf(trans_seasadj_wiki_1)

# ACF at seasonal lags shows slow decrease. 
# Non seasonal we see a sharp dropoff in ACF almost immediately after the second lag, suggesting it might be a MA(2)

ggtsdisplay(trans_seasadj_wiki_2)
pacf(trans_seasadj_wiki_2)
acf(trans_seasadj_wiki_2)
## Non seasonal AR 2? Seasonal AR? Seasonal MA (1)? Non seasonal MA(3)? 
```
###Shortlisting of ARIMA Models based of AICc
```{r}
#ASK PROF ON THURSDAY. Update: Prof say can just mention that the graphs got show  some MA or AR orders and that supports our models (e.g. we got MA and AR components)
# Using autoarima
A0 <- auto.arima(ts(seasadj_wiki, frequency = 7)) #(4,1,1)(2,0,2)[7]

# Variations based on the 1 non seasonal differencing
# A1 <- Arima(seasadj_wiki, order=c(2,1,3), seasonal = list(order = c(3,0,3), period = 7))
A2 <- Arima(seasadj_wiki, order=c(2,1,3), seasonal = list(order = c(2,0,3), period = 7))
A3 <- Arima(seasadj_wiki, order=c(2,1,3), seasonal = list(order = c(2,0,2), period = 7))
# Model 4 is problematic, according to R data is not stationary
A4 <- Arima(seasadj_wiki, order=c(3,1,3), seasonal = list(order = c(2,0,2), period = 7))

# Variations based on having both seasonal and non seasonal differencing
A5 <- Arima(seasadj_wiki, order=c(2,1,3), seasonal = list(order = c(2,1,1), period = 7))
A6 <- Arima(seasadj_wiki, order=c(3,1,3), seasonal = list(order = c(2,1,1), period = 7))
# A7 <- Arima(seasadj_wiki, order=c(3,1,3), seasonal = list(order = c(2,1,2), period = 7))

model_AIC = data.frame(model = c("A0", "A1", "A2", "A3", "A4", "A5", "A6","A7"),
                       AIC = c(A0$aic, NA, A2$aic, A3$aic, A4$aic, A5$aic, A6$aic, NA))
# model_AIC = data.frame(model = c("A0", "A1", "A2", "A3", "A4", "A5", "A6","A7"),
#                        AIC = c(A0$aic, A1$aic, A2$aic, A3$aic, 100000, A5$aic, A6$aic, A7$aic))

model_AIC
# Based on this model AIC we are shortlisting the A0, A1, A2,A3 which all have similar AICs 
```

###Checking Residuals & Accuracy of Shortlisted ARIMA Models
```{r}
#Only only that passed were (4,1,3)(2,1,2)[7] and (4,1,3)(1,0,2)[7]

f0 = stlm(msts_train, modelfunction = Arima, order=c(4,1,1),
         seasonal = list(order = c(2,0,2), period = 7), robust = T)

f1 = stlm(msts_train, modelfunction = Arima, order=c(2,1,3),
         seasonal = list(order = c(3,0,3), period = 7), robust = T) 

f2 = stlm(msts_train, modelfunction = Arima, order=c(2,1,3),
         seasonal = list(order = c(2,0,3), period = 7), robust = T) 

f3 = stlm(msts_train, modelfunction = Arima, order=c(2,1,3),
         seasonal = list(order = c(2,0,2), period = 7), robust = T) 

f4 = stlm(msts_train, modelfunction = Arima, order=c(3,1,3),
         seasonal = list(order = c(2,0,2), period = 7), robust = T)

# PROBLEMATIC MODEL - Cannot be estimated
# f5 = stlm(msts_train, modelfunction = Arima, order=c(2,1,3),
         # seasonal = list(order = c(2,1,1), period = 7), robust = T)
# ERROR stating non-finite finite-difference value. This is related to the stationarity of the data according to stackoverflow. Cannot do MLE when the data is not stationary. 

f6 = stlm(msts_train, modelfunction = Arima, order=c(3,1,3), 
         seasonal = list(order = c(2,1,1), period = 7), robust = T) 

f7 = stlm(msts_train, modelfunction = Arima, order=c(3,1,3), 
          seasonal = list(order = c(2,1,2), period = 7), robust = T)


# f7 = stlm(msts_train, modelfunction = Arima, order=c(4,1,2), 
          # seasonal = list(order = c(2,0,2), period = 7), robust = T)

checkresiduals(forecast(f0_test, h = val_size))

checkresiduals(forecast(f0, h = val_size))
checkresiduals(forecast(f1, h = val_size))
checkresiduals(forecast(f2, h= val_size))
checkresiduals(forecast(f3, h = val_size))
checkresiduals(forecast(f4, h = val_size))
# checkresiduals(forecast(f5))
checkresiduals(forecast(f6, h = val_size))
checkresiduals(forecast(f7, h = val_size))

#Only f4, f6, f7 passed the Ljung-Box Test for the train set
accuracy(forecast(f4), msts_test)
# accuracy(forecast(f5), msts_test)
accuracy(forecast(f6), msts_test)
accuracy(forecast(f7), msts_test)

autoplot(msts_wiki_daily, series = "Original")+autolayer(forecast(f4), PI = F, alpha = 0.9, col = "red", series = "Forecast")+ scale_colour_manual(values=c("Original"="black","Forecast"="red"),
                      breaks=c("Original","Forecast"))
```

```{r}

plot_info = function(model){
  autoplot(msts_wiki_daily, series = "Original")+autolayer(forecast(model), PI = F, alpha = 0.9, col = "red", series = "Forecast")+ scale_colour_manual(values=c("Original"="black","Forecast"="red"),
                      breaks=c("Original","Forecast"))  
}

plot_info(b1)

autoplot(msts_wiki_daily, series = "Original")+autolayer(forecast(f0), PI = F, alpha = 0.9, col = "red", series = "Forecast")+ scale_colour_manual(values=c("Original"="black","Forecast"="red"),
                      breaks=c("Original","Forecast"))

autoplot(msts_wiki_daily, series = "Original")+autolayer(forecast(f1), PI = F, alpha = 0.9, col = "red", series = "Forecast")+ scale_colour_manual(values=c("Original"="black","Forecast"="red"),
                      breaks=c("Original","Forecast"))

autoplot(msts_wiki_daily, series = "Original")+autolayer(forecast(f2), PI = F, alpha = 0.9, col = "red", series = "Forecast")+ scale_colour_manual(values=c("Original"="black","Forecast"="red"),
                      breaks=c("Original","Forecast"))

autoplot(msts_wiki_daily, series = "Original")+autolayer(forecast(f3), PI = F, alpha = 0.9, col = "red", series = "Forecast")+ scale_colour_manual(values=c("Original"="black","Forecast"="red"),
                      breaks=c("Original","Forecast"))

autoplot(msts_wiki_daily, series = "Original")+autolayer(forecast(f4), PI = F, alpha = 0.9, col = "red", series = "Forecast")+ scale_colour_manual(values=c("Original"="black","Forecast"="red"),
                      breaks=c("Original","Forecast"))

autoplot(msts_wiki_daily, series = "Original")+autolayer(forecast(f6), PI = F, alpha = 0.9, col = "red", series = "Forecast")+ scale_colour_manual(values=c("Original"="black","Forecast"="red"),
                      breaks=c("Original","Forecast"))

autoplot(msts_wiki_daily, series = "Original")+autolayer(forecast(f7), PI = F, alpha = 0.9, col = "red", series = "Forecast")+ scale_colour_manual(values=c("Original"="black","Forecast"="red"),
                      breaks=c("Original","Forecast"))

autoplot(msts_wiki_daily, series = "Original")+autolayer(forecast(b1), PI = F, alpha = 0.9, col = "red", series = "Forecast")+ scale_colour_manual(values=c("Original"="black","Forecast"="red"),
                      breaks=c("Original","Forecast"))

autoplot(msts_wiki_daily, series = "Original")+autolayer(forecast(b2), PI = F, alpha = 0.9, col = "red", series = "Forecast")+ scale_colour_manual(values=c("Original"="black","Forecast"="red"),
                      breaks=c("Original","Forecast"))
```

## Random Forest
```{r}
matrix_train <- data.frame(day1 = views_train_matrix[,1],
                           day2 = views_train_matrix[,2],
                           day3 = views_train_matrix[,3],
                           day4 = views_train_matrix[,4],
                           day5 = views_train_matrix[,5],
                           day6 = views_train_matrix[,6],
                           day7 = views_train_matrix[,7],
                           pred = views_train_matrix[,8])


matrix_test <- data.frame(day1 = views_matrix[,1],
                           day2 = views_matrix[,2],
                           day3 = views_matrix[,3],
                           day4 = views_matrix[,4],
                           day5 = views_matrix[,5],
                           day6 = views_matrix[,6],
                           day7 = views_matrix[,7])

rf_model <- randomForest(pred ~ ., data = matrix_train,
                         ntree = 1000, mtry = 2, nodesize = 5, importance = T)


y_pred <- predict(rf_model, matrix_test) 
y_test = y_pred[(length(y_pred)-length(views_test)+1):length(y_pred)]
rmse_rf <- sqrt(mean((views_test-y_test)^2))
rmse_rf

x_axes = seq(1:(length(views_test)))
plot(x_axes, views_test, type="l", col="red", lwd=2)
lines(x_axes, y_test, col="blue",lwd=2)
legend("topleft", legend=c("y-original", "y-predicted"),
       col=c("red", "blue"), lty=1,cex=0.8)

```


## Facebook Prophet
```{r}
prophet_data = rename(daily_data, y = views, ds = date)
prophet_train = slice_head(prophet_data, n = train_size)
prophet_test = slice_tail(prophet_data, n = val_size)

model = prophet(prophet_train, daily.seasonality = FALSE)

# Prediction
future = make_future_dataframe(model, periods = dim(prophet_test)[1])
prophet_forecast = predict(model, future) %>% slice_tail(., n = dim(prophet_test)[1])
accuracy(prophet_test$y, prophet_forecast$yhat)

# accuracy(forecast(f7, h=dim(prophet_test[1])), msts_test)
# accuracy(forecast(f7), msts_test)

# Evaluation
# prophet_plot_components(model, prophet_forecast)
# autoplot(prophet_forecast$yhat)

x_axes = seq(1:(length(prophet_forecast$yhat)))
plot(x_axes, prophet_test$y, type="l", col="red", lwd=2)
lines(x_axes, prophet_forecast$yhat, col="blue",lwd=2)
legend("topleft", legend=c("y-original", "y-predicted"),
       col=c("red", "blue"), lty=1,cex=0.8)
```

## ARIMA-X with 2020 Outlier
```{r}
daily_data$y2020 = sapply(daily_data$date, function(x) {
  if (year(x) ==2020) {return(1)}
  return(0)
})

adj2020 = daily_data$y2020[1:train_size]
# length(daily_data$y2020[(train_size+1):(train_size+test_size)])

testA1 = auto.arima(ts(seasadj_wiki, frequency = 7), xreg = adj2020)
```

```{r}
f4_adj = stlm(msts_train, modelfunction = Arima, order=c(3,1,3),
         seasonal = list(order = c(2,0,2), period = 7), robust = T, xreg = y2020)

accuracy(forecast(f4_adj), msts_test)

?stlm()
```


