# Libraries
```{r include=FALSE}
library(jsonlite)
library(lubridate)
library(forecast)
library(urca)
library(TSstudio)
library(prophet)
library(randomForest)
library(keras) # for deep learning
library(tidyverse) # general utility functions
library(caret) # machine learning utility functions
library(randomForest) # for RF modelling
source('dataprocessing.R')
```

```{r}
# get_sequential_data(3,10)
```


### Importing data
```{r}
rm(list = ls())
daily_data = read_csv('trainvaldf.csv', show_col_types = FALSE)
views = daily_data$views / 1000000
test_size = ceiling(dim(daily_data)[1] * 0.2)
train_size = dim(daily_data)[1] - test_size

# Putting data into TS Object
msts_wiki_daily = daily_data$views %>% msts(seasonal.periods = c(7, 365), start =c(2015, as.numeric(format(daily_data$date[1], "%j"))))
# Creating Train and Test Set
test_size = ceiling(dim(daily_data)[1] * 0.2)
train_size = dim(daily_data)[1] - test_size
msts_split <- ts_split(msts_wiki_daily, sample.out = 365)
msts_train = msts_split$train
msts_test = msts_split$test
```


### Creating Training Data Set
```{r}
max_len <- 365 # the number of previous examples we'll look at
views_train <- views[1:(length(views)-365+1)]
views_test <- views[(length(views)-365+1):length(views)]

normalization = c(mean(views_train), sd(views_train))

views_norm <- (views - normalization[1]) / normalization[2]
train_norm = views_norm[1:(length(views)-365+1)]
test_norm = views_norm[(length(views)-365+1):length(views)]

# Cut the text in overlapping sample sequences of max_len characters
# get a list of start indexes for our (overlapping) chunks
start_indexes_x <- seq(1, (length(views_norm) - max_len - max_len + 1), by = 1)
start_indexes_y <- seq(1+max_len, length(views_norm) - max_len + 1, by = 1)

start_indexes_test <- seq((length(views_norm) - max_len - max_len + 1), length(views_norm) - max_len , by = 1)

# create an empty matrix to store our data in
views_train_matrix_x <- matrix(nrow = length(start_indexes_x), ncol = max_len)
views_train_matrix_y <- matrix(nrow = length(start_indexes_y), ncol = max_len)
views_test_matrix <- matrix(nrow = length(start_indexes_test), ncol = max_len)

# fill our matrix with the overlapping slices of our dataset
for (i in 1:length(start_indexes_x)){
  views_train_matrix_x[i,] <- views_norm[start_indexes_x[i]:(start_indexes_x[i] + max_len-1)]
}

for (i in 1:length(start_indexes_y)){
  views_train_matrix_y[i,] <- views_norm[start_indexes_y[i]:(start_indexes_y[i] + max_len-1)]
}

for (i in 1:length(start_indexes_test)){
  views_test_matrix[i,] <- views_norm[start_indexes_test[i]:(start_indexes_test[i] + max_len-1)]
}
# remove na's if you have them
if(anyNA(views_train_matrix_x)){
    views_train_matrix_x <- na.omit(views_train_matrix_x)
}
if(anyNA(views_train_matrix_y)){
    views_train_matrix_y <- na.omit(views_train_matrix_y)
}
if(anyNA(views_test_matrix)){
    views_test_matrix <- na.omit(views_test_matrix)
}
```



## FeedForward Neural Network
### FeedForward NN Training
```{r}

# NN_model = stlm(msts_train, modelfunction=forecast::nnetar)
# autoplot(msts_wiki_daily)+autolayer(forecast(NN_model, h = 482))
# accuracy(forecast(NN_model, h= 482), msts_test)
wiki_1 = diff(msts_train)
wiki_7 = diff(msts_train,7)
wiki1nn = nnetar(wiki_1)
forecastedgrowth1 = forecast(wiki1nn, 482)
wiki7nn = nnetar(wiki_7)
forecastedgrowth7 = forecast(wiki7nn, 482)
#cbind(tail(wiki_1, length(wiki_7)), wiki_7)
NN_model2 = nnetar(tail(msts_train, length(wiki_1)), xreg = wiki_1 )
#cbind(tail(forecastedgrowth1$x, 482), tail(forecastedgrowth7$x, 482))

```

### FeedForward NN Testing
```{r}
NN_forecast = forecast(NN_model2, h = 482, xreg = tail(forecastedgrowth1$x, 482) )
autoplot(msts_test)+autolayer(NN_forecast)
autoplot(msts_wiki_daily)+autolayer(NN_forecast, alpha = 0.7, col = 'red')
autoplot(NN_forecast)
accuracy(NN_forecast,msts_test)
```

## RNN
### RNN Training
```{r}
# set a random seed for reproducability
set.seed(123)

batch_size <- 16 # Size of batch for mini-batch gradient descent. number of sequences to look at at one time during training
total_epochs <- 50 # how many times we'll look @ the whole dataset while training our model

model <- NULL

model <- keras_model_sequential() %>%   
  layer_dense(units=max_len, input_shape=c(max_len, 1), activation="relu") %>%
  layer_simple_rnn(units=64, activation = "relu") %>%  
  layer_dense(units=20, activation="relu") %>%
  layer_dense(units=1, activation = "linear")
 
model %>% compile(loss = 'mse',
                  optimizer = 'adam',
                  metrics = list("mean_squared_error"))
model %>% summary()

model %>% fit(views_train_matrix_x,views_train_matrix_y, epochs=total_epochs, batch_size= batch_size, shuffle = FALSE,validation_split = 0.3, callbacks = list(callback_early_stopping(monitor = 'val_loss', patience = 6, verbose = 1, restore_best_weights = T)))
```


### RNN Testing
```{r}
y_pred = model %>% predict(views_test_matrix)
#y_pred = y_pred * normalization[2] + normalization[1]
#y_test = y_pred[(length(y_pred)-length(views_train_matrix_test)+1):length(y_pred)]
y_test <- views[(length(views)-365+1):length(views)]
x_axes = seq(1:(length(y_pred)))
plot(x_axes, y_test, type="l", col="red", lwd=2)
lines(x_axes, y_pred, col="blue",lwd=2)
legend("topleft", legend=c("y-original", "y-predicted"),
        col=c("red", "blue"), lty=1,cex=0.8)


rmse_rnn <- sqrt(mean((y_test-y_pred)^2))
rmse_rnn
```

## LSTM
### LSTM Model Training
```{r}
# set a random seed for reproducability
set.seed(123)

batch_size <- 16 # number of sequences to look at at one time during training
total_epochs <- 50 # how many times we'll look @ the whole dataset while training our model

model <- NULL


model <-  keras_model_sequential() %>%   
  layer_dense(units=max_len, input_shape=c(max_len, 1), activation="relu") %>%  
  layer_lstm(units=64, activation = "relu") %>%
  layer_dense(units=20, activation = "relu") %>%
  layer_dense(units=1, activation = "linear")
 
model %>% compile(loss = 'mse',
                  optimizer = 'adam',
                  metrics = list("mean_squared_error"))x
model %>% summary()

model %>% fit(X_train,y_train, epochs=total_epochs, batch_size= batch_size, shuffle = FALSE,validation_split = 0.2, callbacks = list(callback_early_stopping(monitor = 'val_loss', patience = 6, verbose = 0, restore_best_weights = T)))

#lstm_outcome <- model %>% evaluate(X_test, y_test, verbose = 0)

y_pred = model %>% predict(X)
y_test = y_pred[(length(y_pred)-length(views_test)+1):length(y_pred)]
x_axes = seq(1:(length(views_test)))
plot(x_axes, views_test, type="l", col="red", lwd=2)
lines(x_axes, y_test, col="blue",lwd=2)
legend("topleft", legend=c("y-original", "y-predicted"),
        col=c("red", "blue"), lty=1,cex=0.8)

rmse_lstm <- sqrt(mean((views_test-y_test)^2))
rmse_lstm

# rmse_lstm <- sqrt(lstm_outcome[2])
# rmse_lstm
# y_pred = model %>% predict(X_test)
# x_axes = seq(1:length(y_pred))
# plot(x_axes, y_test, type="l", col="red", lwd=2)
# lines(x_axes, y_pred, col="blue",lwd=2)
# legend("topleft", legend=c("y-original", "y-predicted"),
#         col=c("red", "blue"), lty=1,cex=0.8)

```

## Prof's Code
```{r}
lstmforecast = function(yvariable, forecasthorizon){
  normalization = c(mean(yvariable), sd(yvariable))
  train_norm = (yvariable - normalization[1]) / normalization[2]
  train_norm = as.matrix(train_norm)
  
  x_train_data = t(sapply(1:(length(train_norm) - forecasthorizon - forecasthorizon + 1), function(x) train_norm[x:(x + forecasthorizon - 1), 1] ))
  x_train_arr = array(data = as.numeric(unlist(x_train_data)), dim = c( nrow(x_train_data), forecasthorizon,   1   )  )
  y_train_data = t(sapply( (1 + forecasthorizon):(length(train_norm) - forecasthorizon + 1), function(x) train_norm[x:(x + forecasthorizon - 1)] ))
  y_train_arr = array( data = as.numeric(unlist(y_train_data)),  dim = c(  nrow(y_train_data),  forecasthorizon,   1    ) )
  x_test = yvariable[(nrow(train_norm) - 2*forecasthorizon + 1):(nrow(train_norm) - forecasthorizon + 1)]
  x_test_scaled = (x_test - normalization[1]) / normalization[2]
  x_pred_arr = array(data = x_test_scaled,  dim = c( 1, forecasthorizon,  1 ) )
  
  #####LSTM SETTINGS COPY PASTED FROM DOCS WITHOUT MODIFICATION###########
  lstm_model = keras_model_sequential()
  lstm_model %>%
    layer_lstm(units = 50, # size of the layer
               batch_input_shape = c(1, forecasthorizon, 1), # batch size, timesteps, features
               return_sequences = TRUE,
               stateful = TRUE) %>%
    # layer_dense(units=20, activation = "relu") %>%
    # fraction of the units to drop for the linear transformation of the inputs
    layer_dropout(rate = 0.5) %>%
    layer_lstm(units = 50,
               return_sequences = TRUE,
               stateful = TRUE) %>%
    layer_dropout(rate = 0.5) %>%
    time_distributed(keras::layer_dense(units = 1))
  
  lstm_model %>%compile(loss = 'mae', optimizer = 'adam', metrics = 'mean_squared_error')     
  lstm_model %>% fit(x = x_train_arr, y = y_train_arr, batch_size = 1, epochs = 20,shuffle = FALSE, validation_split = 0.2, callbacks = list(callback_early_stopping(monitor = 'val_loss', patience = 6, verbose = 1, restore_best_weights = T)) )    
  lstm_forecast = lstm_model %>% predict(x_pred_arr, batch_size = 1) %>%.[, , 1]  #LINE X
  lstm_forecast = lstm_forecast * normalization[2] + normalization[1]
  
  return(lstm_forecast)   
}



##EDIT THESE SETTINGS
var = msts_wiki_daily
forecasthorizon = 365

##DO NOT REALLY NEED TO EDIT THIS PART######
forecast = lstmforecast(var,forecasthorizon)
forecastedseries = ts(append(var[1:(length(var)-forecasthorizon)], forecast))
autoplot(forecastedseries) + autolayer(ts(var))

y_pred <- forecastedseries[(length(forecastedseries) - forecasthorizon + 1) : length(forecastedseries)]
y_test <- msts_split$test
sqrt(mean((y_test - y_pred)^2))

x_axes = seq(1:forecasthorizon)
plot(x_axes, y_pred, type="l", col="red", lwd=2)
lines(x_axes, y_test, col="blue",lwd=2)
legend("topleft", legend=c("y-original", "y-predicted"),
        col=c("red", "blue"), lty=1,cex=0.8)

```

```{r}
rnnforecast = function(yvariable, forecasthorizon){
  normalization = c(mean(yvariable), sd(yvariable))
  train_norm = (yvariable - normalization[1]) / normalization[2]
  train_norm = as.matrix(train_norm)
  
  x_train_data = t(sapply(1:(length(train_norm) - forecasthorizon - forecasthorizon + 1), function(x) train_norm[x:(x + forecasthorizon - 1), 1] ))
  x_train_arr = array(data = as.numeric(unlist(x_train_data)), dim = c( nrow(x_train_data), forecasthorizon,   1   )  )
  y_train_data = t(sapply( (1 + forecasthorizon):(length(train_norm) - forecasthorizon + 1), function(x) train_norm[x:(x + forecasthorizon - 1)] ))
  y_train_arr = array( data = as.numeric(unlist(y_train_data)),  dim = c(  nrow(y_train_data),  forecasthorizon,   1    ) )
  x_test = yvariable[(nrow(train_norm) - 2*forecasthorizon + 1):(nrow(train_norm) - forecasthorizon + 1)]
  x_test_scaled = (x_test - normalization[1]) / normalization[2]
  x_pred_arr = array(data = x_test_scaled,  dim = c( 1, forecasthorizon,  1 ) )
  
  #####LSTM SETTINGS COPY PASTED FROM DOCS WITHOUT MODIFICATION###########
  rnn_model = keras_model_sequential()
  rnn_model %>%
    layer_simple_rnn(units = 64, # size of the layer
               batch_input_shape = c(1, forecasthorizon, 1), # batch size, timesteps, features
               return_sequences = TRUE,
               stateful = TRUE) %>%
    layer_dense(units=20, activation = "relu") %>%
    # fraction of the units to drop for the linear transformation of the inputs
    layer_dropout(rate = 0.5) %>%
    layer_simple_rnn(units = 64,
               return_sequences = TRUE,
               stateful = TRUE) %>%
    layer_dropout(rate = 0.5) %>%
    layer_dense(units=20, activation = "relu") %>%
    time_distributed(keras::layer_dense(units = 1))
  
  rnn_model %>%compile(loss = 'mae', optimizer = 'adam', metrics = 'mean_squared_error')     
  rnn_model %>% fit(x = x_train_arr, y = y_train_arr, batch_size = 1, epochs = 20,shuffle = FALSE, validation_split = 0.2, callbacks = list(callback_early_stopping(monitor = 'val_loss', patience = 6, verbose = 1, restore_best_weights = T)) )    
  rnn_forecast = rnn_model %>% predict(x_pred_arr, batch_size = 1) %>%.[, , 1]  #LINE X
  rnn_forecast = rnn_forecast * normalization[2] + normalization[1]
  
  return(rnn_forecast)   
}



##EDIT THESE SETTINGS
var = msts_split$train
forecasthorizon = 365

##DO NOT REALLY NEED TO EDIT THIS PART######
forecast = lstmforecast(var,forecasthorizon)
forecastedseries = ts(append(var[1:(length(var)-forecasthorizon)], forecast))
autoplot(forecastedseries) + autolayer(ts(var))

y_pred <- forecastedseries[(length(forecastedseries) - forecasthorizon + 1) : length(forecastedseries)]
y_test <- msts_split$test
sqrt(mean((y_test - y_pred)^2))

x_axes = seq(1:forecasthorizon)
plot(x_axes, y_pred, type="l", col="red", lwd=2)
lines(x_axes, y_test, col="blue",lwd=2)
legend("topleft", legend=c("y-original", "y-predicted"),
        col=c("red", "blue"), lty=1,cex=0.8)

```
