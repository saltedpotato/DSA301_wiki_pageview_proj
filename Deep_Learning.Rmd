# Libraries
```{r}
library(jsonlite)
library(lubridate)
library(forecast)
library(urca)
library(TSstudio)
library(prophet)
library(randomForest)
library(keras) # for deep learning
library(tidyverse) # general utility functions
library(caret) # machine learning utility functions
library(randomForest) # for RF modelling 
```

### Importing data
```{r}
rm(list = ls())
daily_data = read_csv('dailydata.csv', show_col_types = FALSE)
daily_data$views = daily_data$views / 1000000
test_size = ceiling(dim(daily_data)[1] * 0.2)
train_size = dim(daily_data)[1] - test_size
```


### Creating Training Data Set
```{r}
max_len <- 7 # the number of previous examples we'll look at
views <- daily_data$views
views_train <- views[1:floor(length(views) * 0.8)]
views_test <- views[(train_size+1):length(views)]

# Cut the text in overlapping sample sequences of max_len characters
# get a list of start indexes for our (overlapping) chunks
start_indexes <- seq(1, length(views_train) - (max_len + 1), by = 1)

# create an empty matrix to store our data in
views_train_matrix <- matrix(nrow = length(start_indexes), ncol = max_len + 1)

# fill our matrix with the overlapping slices of our dataset
for (i in 1:length(start_indexes)){
  views_train_matrix[i,] <- views_train[start_indexes[i]:(start_indexes[i] + max_len)]
}

# remove na's if you have them
if(anyNA(views_train_matrix)){
    views_train_matrix <- na.omit(views_train_matrix)
}

X_train <- views_train_matrix[,-ncol(views_train_matrix)]
y_train <- views_train_matrix[,ncol(views_train_matrix)]
```

### Training the model
```{r}
# set a random seed for reproducability
set.seed(123)

batch_size <- 16 # Size of batch for mini-batch gradient descent. number of sequences to look at at one time during training
total_epochs <- 50 # how many times we'll look @ the whole dataset while training our model

model <- NULL

model <- keras_model_sequential() %>%   
  layer_dense(units=max_len, input_shape=c(max_len, 1), activation="relu") %>%
  layer_simple_rnn(units=6, activation = "relu") %>%  
  layer_dense(units=1, activation = "linear")
 
model %>% compile(loss = 'mse',
                  optimizer = 'adam',
                  metrics = list("mean_squared_error"))
model %>% summary()

model %>% fit(X_train,y_train, epochs=total_epochs, batch_size= batch_size, shuffle = FALSE,validation_split = 0.2, callbacks = list(callback_early_stopping(monitor = 'val_loss', patience = 6, verbose = 0, restore_best_weights = T)))
```

### Creating Test Data Set
```{r}
start_indexes_all <- seq(1, length(views) - (max_len + 1), by = 1)

# create an empty matrix to store our data in
views_matrix <- matrix(nrow = length(start_indexes_all), ncol = max_len + 1)

# fill our matrix with the overlapping slices of our dataset
for (i in 1:length(start_indexes_all)){
  views_matrix[i,] <- views[start_indexes_all[i]:(start_indexes_all[i] + max_len)]
}

# remove na's if you have them
if(anyNA(views_matrix)){
    views_matrix <- na.omit(views_matrix)
}

X <- views_matrix[,-ncol(views_matrix)]
y <- views_matrix[,ncol(views_matrix)]
# training_index <- seq(1,floor(0.8*length(views) - 8))
# 
# # training data
# X_train <- array(X[training_index,], dim = c(length(training_index), max_len, 1))
# y_train <- y[training_index]
# 
# # testing data
# X_test <- array(X[-training_index,], dim = c(length(y) - length(training_index), max_len, 1))
# y_test <- y[-training_index]
```

### Predicting using RNN 

```{r}
y_pred = model %>% predict(X)
y_test = y_pred[(length(y_pred)-length(views_test)+1):length(y_pred)]
x_axes = seq(1:(length(views_test)))
plot(x_axes, views_test, type="l", col="red", lwd=2)
lines(x_axes, y_test, col="blue",lwd=2)
legend("topleft", legend=c("y-original", "y-predicted"),
        col=c("red", "blue"), lty=1,cex=0.8)


rmse_rnn <- sqrt(mean((views_test-y_test)^2))
rmse_rnn
```

