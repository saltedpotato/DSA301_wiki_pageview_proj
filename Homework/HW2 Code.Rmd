---
title: "R Notebook"

---
Q1
```{r Q1 install libraries}
library(fpp2)
library(forecast)
library(urca)
```

```{r Q1 (a)}
autoplot(ma(usmelec,12,centre = TRUE))

```

```{r Q1 (b)}
boxcox_usmelec <- BoxCox(usmelec,BoxCox.lambda(usmelec))
autoplot(boxcox_usmelec)
```
From part (a), we can see that the variance of the autoplot is not constant, and therefore BoxCox transformation should be employed.


```{r Q1 (c)}
boxcox_usmelec %>%
  ur.kpss()%>%
  summary()

nsdiffs(boxcox_usmelec) #1
ndiffs(diff(boxcox_usmelec,12)) #1

boxcox_usmelec%>%
  diff(lag = 12) %>%
  diff() %>%
  ur.kpss()%>%
  summary()

acf(boxcox_usmelec)
acf(diff(diff(boxcox_usmelec,12)))
```
From the KPSS test, we can see that the data is not stationary with a test statistic of 7.8337, which is larger than the critical values in 1pct. Hence, we check if differencing is needed using nsdiffs and ndiffs. After differencing, the KPSS test statistic is lowered to 0.0167, which is lower than the critical value in 10pct.

```{r Q1 (d)}
ggtsdisplay(boxcox_usmelec)
ggtsdisplay(diff(diff(boxcox_usmelec,12)))

m1 <- auto.arima(usmelec, lambda = "auto", biasadj = T) #(1,1,3)(2,1,1)[12]
m2 <- Arima(usmelec, order = c(2,1,3), seasonal = c(3,1,1), lambda = "auto", biasadj = T)
m3 <- Arima(usmelec, order = c(2,1,3), seasonal = c(2,1,1), lambda = "auto", biasadj = T)
m4 <- Arima(usmelec, order = c(1,1,3), seasonal = c(2,1,2), lambda = "auto", biasadj = T)
m5 <- Arima(usmelec, order = c(1,1,2), seasonal = c(2,1,1), lambda = "auto", biasadj = T)
m6 <- Arima(usmelec, order = c(1,1,1), seasonal = c(2,1,1), lambda = "auto", biasadj = T)
m7 <- Arima(usmelec, order = c(1,1,1), seasonal = c(3,1,1), lambda = "auto", biasadj = T)

model_AIC = data.frame(model = c("m1", "m2", "m3", "m4", "m5", "m6", "m7"),
                       AIC = c(m1$aic, m2$aic, m3$aic, m4$aic, m5$aic, m6$aic, m7$aic))
model_AIC

```
Best model based on AIC is m6 with lowest AIC value of -5082.634 compared to the other models.

```{r Q1 (e)}
checkresiduals(m1)
checkresiduals(m5)
checkresiduals(m6)

```

Since m5 has an AIC close to m6, we will consider it as well. 

From the checkresiduals test, all models both pass the Ljung-box test with p-value of 0.07454, 0.08662, 0.08318 respectively at 5% sig. level.
Hence, all models have residuals resembling resembling noise at 5% sig level.


```{r Q1 (f)}
latest_data <- read.csv("HW2 Q1 Cleaned Data.csv")
latest_data_ts <- ts(latest_data["Amount"], start = c(2013,7), deltat = 1/12)
accuracy(forecast(m1,h = 15*12), latest_data_ts)
accuracy(forecast(m5,h = 15*12), latest_data_ts)
accuracy(forecast(m6,h = 15*12), latest_data_ts)

autoplot(forecast(m1,h = 15*12), PI = F)
autoplot(forecast(m1,h = 15*12), PI = T)

```

From the accuracy tests, m1 attained the best RMSE score in the train and test set. Hence, m1 is the best model to use for prediction.

Q1 (g)
Since the predictions of m7 is reasonably accurate, the general predictability of the trend, seasonality, and heteroskedastic attributes of the time series makes for a more accurate forecast. With such long and stable patterns in the data, predictions up to a decade would not be unseasonable. Afterwards, predictions may no longer be accurate, and should no longer be used.

